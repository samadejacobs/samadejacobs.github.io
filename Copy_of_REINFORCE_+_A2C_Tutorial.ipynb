{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of REINFORCE_+_A2C_Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "91d709cfa9654b11ac90969b25150c81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cbf02aadb0fb4358aa5b89af9f81aa1a",
              "IPY_MODEL_89326d502b334d33b1026d8d90b22dd0"
            ],
            "layout": "IPY_MODEL_0cae9ff624fd4f92a0d0afb7a6738406"
          }
        },
        "0cae9ff624fd4f92a0d0afb7a6738406": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "cbf02aadb0fb4358aa5b89af9f81aa1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Epoch 100: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4336f7b3d62f479ba66ee90eef41257d",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_59177c2ad7a64ecc8b3454dad1c5fac6",
            "value": 10
          }
        },
        "89326d502b334d33b1026d8d90b22dd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b4b77a07d86484197877fc0158ceae4",
            "placeholder": "​",
            "style": "IPY_MODEL_e6775ba11c8c4051876da297ee037e61",
            "value": " 10/10 [32:37&lt;00:00, 195.71s/it, Entropy=0.528, KL=-.00103, MaxEpReturn=500, MeanEpLength=354, MeanEpReturn=354, MinEpReturn=111, PolicyLoss=25.2, loss=40.506, v_num=3]"
          }
        },
        "59177c2ad7a64ecc8b3454dad1c5fac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "4336f7b3d62f479ba66ee90eef41257d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6775ba11c8c4051876da297ee037e61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b4b77a07d86484197877fc0158ceae4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0a781cd93374b3f88158eb60bc01238": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_810e166708814013be4bae3fd8f6c6a9",
              "IPY_MODEL_ce8d18dca93746caba6d355f4eb678a3"
            ],
            "layout": "IPY_MODEL_e5f25ca13bee4937a97abbf25ff8faa6"
          }
        },
        "e5f25ca13bee4937a97abbf25ff8faa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "810e166708814013be4bae3fd8f6c6a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Epoch 100: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41524da1be7945bd82f25d60466aee34",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a7a7193f67e447929e48bcb3913794b9",
            "value": 10
          }
        },
        "ce8d18dca93746caba6d355f4eb678a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8be6d638acfd4219989392d078bdf3f0",
            "placeholder": "​",
            "style": "IPY_MODEL_7a88dcb786c846f8b18092382602e92c",
            "value": " 10/10 [00:03&lt;00:00,  2.53it/s, DeltaPolLoss=-.000336, DeltaValLoss=-6.76, Entropy=0.492, KL=-.000396, MaxEpReturn=500, MeanEpLength=442, MeanEpReturn=442, MinEpReturn=171, PolicyLoss=0.119, ValueLoss=1.14e+3, loss=550.986, v_num=10]"
          }
        },
        "a7a7193f67e447929e48bcb3913794b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "41524da1be7945bd82f25d60466aee34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a88dcb786c846f8b18092382602e92c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8be6d638acfd4219989392d078bdf3f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samadejacobs/samadejacobs.github.io/blob/master/Copy_of_REINFORCE_%2B_A2C_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "id": "ynBj9bDCp95g"
      },
      "source": [
        "# A Tutorial on REINFORCE and A2C Policy Gradient Algorithms\n",
        "\n",
        "During this, we'll focus on connecting theory to code. \n",
        "\n",
        "We'll begin with some fundamental RL concepts and terminology, and then quickly move on to covering the algorithms.\n",
        "\n",
        "A quick note as well, extra information is labelled under subsections titled **Extra Detail:**. These aren't necessary to read but are instead there for the interested reader who wants a bit more depth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "true",
        "colab_type": "text",
        "id": "Jn1yQikXruJW"
      },
      "source": [
        "## Section 1: Some RL Background\n",
        "---\n",
        "### 1.1: What are Markov Decision Processes (MDPs)?\n",
        "---\n",
        "MDPs are the typical problem formulation for RL agents. \n",
        "\n",
        "![mdp](https://www.kdnuggets.com/images/reinforcement-learning-fig1-700.jpg)\n",
        "\n",
        "#### Extra Detail:\n",
        "\n",
        "MDPs consist of an interaction loop between the agent and the environment, where on the first step, the agent receives some state from the environment, takes an action, and then receives a new state and a reward. The agent then takes in this new state and reward, and chooses the next action. The interaction continues until the episode (an episode is a period of agent-environment interaction) ends.\n",
        "\n",
        "Some MDPs are not episodic, and instead are continuing (AKA infinite-horizon) tasks. We only look at episodic MDPS here.\n",
        "\n",
        "### 1.2: Reward and Return:\n",
        "---\n",
        "In RL, the reward is used to define the goal that you want to solve. At each step, the RL agent receives a single scalar reward from the environment. The agent's goal is to maximize the reward it receives.\n",
        "\n",
        "Return is calculated as the discounted cumulative sum of all reward earned over an episode.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "id": "giok1Xju8zAM"
      },
      "source": [
        "### Extra Detail:\n",
        "\n",
        "The simplest return formulation is just to take the (not discounted) sum over all reward earned during an episode. However, this doesn't work for continuing tasks, because as the horizon goes to infinity, so does the return earned. Let's write this out mathematically, denote the return as $G_t$, $t$ is the step in the environment, and $r_t$ is the reward given at step $t$. Then, the sum of the return is:\n",
        "\n",
        "$G_t \\stackrel{.}{=} \\sum_{t=0}^{t} r_t$\n",
        "\n",
        "We get around the return going to infinity by introducing a discount factor on the return. The discount factor is represented by $\\gamma$ and is frequently set to a value between 0.95 and 0.99. The discounted return is written as:\n",
        "\n",
        "$G_t \\stackrel{.}{=} \\sum_{i=0}^{\\infty} \\gamma^i r_{i+t+1}$\n",
        "\n",
        "Defining it this way makes it a telescoping sum, so as $t \\rightarrow \\infty$, the reward goes to zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "true",
        "colab_type": "text",
        "id": "hfo599eVhK4U"
      },
      "source": [
        "## Policies and Value functions\n",
        "\n",
        "### Policies\n",
        "\n",
        "A policy (denoted by $\\pi$) is a mapping from:\n",
        "- A state ($s$) to an action ($a$) (a deterministic policy): such that $\\pi \\rightarrow a$. Or it maps from ...\n",
        "- A state to a probability distribution over actions (a stochastic policy) such that $\\pi(a|s) = \\mathbb{P}_\\pi [A = a | S = s]$\n",
        "\n",
        "In either case, our goal is to find an optimal policy so that we can be sure that whatever the state of our environment is, we are acting optimially!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "id": "cLXvr8F_iLzT"
      },
      "source": [
        "### Extra Detail:\n",
        "\n",
        "A stochastic policy tells us that the output of the policy (the action) is conditioned on the state $s$. $\\mathbb{P}_\\pi [A = a | S = s]$ is the way of saying that the probability of action $a$ being $A$ is dependent on $s$ equaling $S$. A deterministic policy is more straightforward and maps directly from state to action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "id": "SD8n6fBpiuzy"
      },
      "source": [
        "### Value functions\n",
        "\n",
        "A state's value is determined by how much reward is expected to follow it. If a state is followed by a lot of reward, it must be good, and therefore should have a high value. Value functions are always defined with respect to policies. This is because how much reward should follow a state is influenced by how the policy behaves. If the policy is an optimal or near-optimal one, then the value function associated with that policy will predict different expected returns than a value function associated with a poorly performing policy.\n",
        "\n",
        "The state-value function, $V_\\pi(s)$, is a mapping from a state to an expectation of how much reward should follow that state. It assumes that the actor in the environment will behave in an on-policy manner (all future actions are true to the current policy) for all remaining steps into the future.\n",
        "\n",
        "The action-value function $Q_\\pi(s, a)$ maps from a state and the action taken in that state to an expectation of how much reward should follow that state-action pair. It assumes that the action $a$ may or may not have been on-policy but that all other actions taken into the future will be on-policy.\n",
        "\n",
        "Here, our implementations will only use state-value functions. But, work like the famous Atari-playing DQN, and other, more advanced algorithms make use of the Q-value function instead of the state-value function.\n",
        "\n",
        "**Relationship between optimal Q-function and optimal policy**:\n",
        "\n",
        "When the optimal policy is in state $s$, it chooses the action $a$ that gives the maximum expected future return. Because of this, when we have the optimal q-function, we can get the optimal policy by:\n",
        "\n",
        "$a^*(s) = \\underset{a}{argmax} \\space q^*(s,a)$\n",
        "\n",
        "The * denotes optimality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "id": "uALAhQLulIBv"
      },
      "source": [
        "### Extra Detail:\n",
        "\n",
        "For those who really want to see the equations for the state-value and action-value functions, I've included them here.\n",
        "\n",
        "State-value function:\n",
        "\n",
        "$V_\\pi(s) \\stackrel{.}{=} \\mathbb{E}_\\pi [G_t | s_t = s] = \\mathbb{E}_\\pi [\\sum_{i=1}^\\infty \\gamma^i r_{i+t+1}| s_t = s]$\n",
        "\n",
        "Notice that the $G_t$ and its expansion on the right hand side are from the Reward and Return: Extra Detail section above. Since our value function is maximizing expected future return, we can use those definitions again here.\n",
        "\n",
        "Action-value function:\n",
        "\n",
        "$Q_\\pi(s, a) \\stackrel{.}{=} \\mathbb{E}_\\pi [G_t | s_t = s, a_t = a] = \\mathbb{E}_\\pi [\\sum_{i=1}^\\infty \\gamma^i r_{i+t+1}| s_t = s, a_t = a]$\n",
        "\n",
        "Again, the definitions from reward and return are used here.\n",
        "\n",
        "To dig deeper, read about the Bellman equations, which demonstrate a nice, recursive, self-consistent set of equations for the value of a current state and the values of following states. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "id": "wxmSKIpeo68N"
      },
      "source": [
        "## A brief bonus: exploration/exploitation tradeoff\n",
        "\n",
        "Many people have heard of the exploration/exploitation tradeoff in RL. Often, when we are dealing with a deterministic policy, we force it to explore by either injecting some noise into the action, or by randomly sampling actions occasionally. There are also other, more sophisticated exploration methods.\n",
        "\n",
        "Today, we won't worry about any of that since we are dealing with policy gradient methods. These algorithms output a probability distribution over actions, and at each timestep, we sample from that distribution to get our next action. Because of this, the exploration factor is effectively built-in to these algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyzKPnQF_pfS",
        "colab_type": "text"
      },
      "source": [
        "### Some quick terminology clarification\n",
        "\n",
        "- Epoch: Refers to collecting one batch of experiences (batch of experiences often called steps per epoch in the environment and training on them.\n",
        "    - i.e. I've set my epoch batch size to 4000, so my agent collects 4000 interactions with the environment and trains on them. Then, for the next epoch, we collect 4000 new interactions, and train on those. And so on.\n",
        "- Minibatch size: Refers to sampling minibatches from the epoch batch of experiences and training on each minibatch.\n",
        "    - i.e. my minibatch size is 100, so I'll sample minibatches of size 100 from the epoch batch of 4000 and train on each minibatch until they're gone."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wIBOBhS_pfS",
        "colab_type": "text"
      },
      "source": [
        "![CartPole](https://camo.githubusercontent.com/7089af78ce27348d2a71698b6913f7656a6713cc/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f3830302f312a6f4d5367325f6d4b677541474b793143363455466c772e676966)\n",
        "\n",
        "The environment we will work on today is the [CartPole-v1](http://gym.openai.com/envs/CartPole-v1/) environment, where the agent's goal is to balance the pole atop the cart. This is a classic RL problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "colab_type": "code",
        "id": "cKghsGkHNxSF",
        "colab": {}
      },
      "source": [
        "# install required packages\n",
        "!pip install -q pytorch-lightning\n",
        "!pip install -q gym\n",
        "!pip install -q pybullet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "colab_type": "code",
        "id": "QElTlmOjN2oT",
        "colab": {}
      },
      "source": [
        "# import needed packages\n",
        "%load_ext tensorboard\n",
        "import pytorch_lightning as pl # provides training framework pytorch-lightning.readthedocs.io\n",
        "import gym # provides RL environments gym.openai.com\n",
        "import pybullet_envs # extra RL environments https://docs.google.com/document/d/10sXEhzFRSnvFcl3XxNGhnD4N2SedqwdAvK3dsihxVUA/edit#heading=h.wz5to0x8kqmr\n",
        "import numpy as np # linear algebra https://numpy.org/\n",
        "import matplotlib.pyplot as plt # plotting https://matplotlib.org/\n",
        "import torch # Neural network utilities pytorch.org/\n",
        "import torch.nn as nn # NN building blocks\n",
        "import torch.nn.functional as F # loss functions, activation functions\n",
        "import torch.optim as optim # optimizers\n",
        "from torch.utils.data import Dataset, DataLoader # dataset utilities\n",
        "from typing import Union, Optional, Any, Tuple, List # type hinting https://docs.python.org/3/library/typing.html\n",
        "import sys # direct printing to stdout or to file https://docs.python.org/3/library/sys.html\n",
        "from argparse import Namespace, ArgumentParser # argument handling https://docs.python.org/3/library/argparse.html\n",
        "from scipy.signal import lfilter # helpful in return discounting https://www.scipy.org/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "id": "bJ89r31YqRQN"
      },
      "source": [
        "## The Humble MLP\n",
        "\n",
        "The MLP (Multi-Layer Perceptron) will be used to parameterize our policies and value functions. We'll set it up to take in whatever input size (observation sizes change depending on your environment) and to map to whatever output size (action space sizes change too). Make the activation function optional so that we can use the output directly for logits of a distribution over actions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "colab_type": "code",
        "id": "wYSAU5llOGMB",
        "colab": {}
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features: int, # observation dimensions\n",
        "        hidden_sizes: tuple, # hidden layer sizes\n",
        "        out_features: int, # action dimensions\n",
        "        activation: callable = nn.Tanh(), # activation function\n",
        "        out_activation: callable = nn.Identity(), # output activation function\n",
        "        out_squeeze: bool = False # whether to apply a squeeze to the output\n",
        "      ):\n",
        "        super().__init__()\n",
        "        layer_sizes = [in_features] + list(hidden_sizes) + [out_features]\n",
        "        self.activation = activation\n",
        "        self.out_activation = out_activation\n",
        "        self.out_squeeze = out_squeeze\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        for i, l in enumerate(layer_sizes[1:]):\n",
        "            self.layers.append(nn.Linear(layer_sizes[i], l))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for l in self.layers[:-1]:\n",
        "            x = self.activation(l(x))\n",
        "\n",
        "        x = self.out_activation(self.layers[-1](x))\n",
        "\n",
        "        if self.out_squeeze:\n",
        "            x = torch.squeeze(x, -1)\n",
        "    \n",
        "        return torch.squeeze(x, -1) if self.out_squeeze else x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "id": "r7os2t4HTzq4"
      },
      "source": [
        "## Actor\n",
        "\n",
        "The Actor class provides a structure to follow when we define our policy. \n",
        "\n",
        "By setting the forward pass in Actor, and afterwards inheriting from Actor when we define a new policy, we only have to worry about defining the ```action_distribution``` and ```logprob_from_distribution``` functions.\n",
        "\n",
        "Since we are mapping from states to a probability distribution over actions, $\\pi(a|s)$, our ```action_distribution``` function takes input states as an argument and should return a distribution over actions. \n",
        "\n",
        "In policy gradient algorithms, the log-probability of a chosen action is frequently used in the loss functions. We write the ```logprob_from_distribution``` function to take in our current policy distribution $\\pi(a|s)$ and the selected action and then return the log-probability of that action under the current policy distribution.\n",
        "\n",
        "The ```forward``` function combines these steps for us. It gets the current policy, and if an action $a$ is input as an argument, it computes the log-probability of that action under the policy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "colab_type": "code",
        "id": "J4mMTX0KQgiZ",
        "colab": {}
      },
      "source": [
        "class Actor(nn.Module):\n",
        "    def action_distribution(self, states):\n",
        "        \"\"\"\n",
        "        Get action distribution conditioned on input states.\n",
        "\n",
        "        Args:\n",
        "          states: Input states (works with one or many) to get an action distribution with respect to.\n",
        "\n",
        "        Returns:\n",
        "          Should return a torch.distributions distribution object.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def logprob_from_distribution(self, policy_distribution, action):\n",
        "        \"\"\"\n",
        "        Calculate log-probability of actions taken under a given policy distribution.\n",
        "\n",
        "        Args:\n",
        "          policy_distribution: A torch.distributions object, the current policy distribution.\n",
        "          action: The action that was taken.\n",
        "\n",
        "        Returns:\n",
        "          Log probability of the input action under input policy.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def forward(self, state, a = None):\n",
        "        \"\"\"\n",
        "        Get policy distribution on input state and, if action is input, get logprob of that action.\n",
        "\n",
        "        Args:\n",
        "\n",
        "        \"\"\"\n",
        "        policy = self.action_distribution(state)\n",
        "        logp_a = None\n",
        "        if a is not None:\n",
        "            logp_a = self.logprob_from_distribution(policy, a)\n",
        "        return policy, logp_a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "id": "s8I9xNcQakPs"
      },
      "source": [
        "## The Categorical Policy\n",
        "\n",
        "In environments with discrete action spaces (i.e. the valid actions are integers (1, 2) for example), we can learn a Categorical distribution over actions, and then sample from that distribution to get an action at each timestep in the environment. This follows our math before of having a stochastic policy map from a state to a distribution over actions, $\\pi(a|s)$.\n",
        "\n",
        "We train our policy network (here, an MLP) to output logits and then paramaterize the action distribution with those logits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "colab_type": "code",
        "id": "AyaiBagcS-t5",
        "colab": {}
      },
      "source": [
        "class CategoricalPolicy(Actor):\n",
        "    \"\"\"\n",
        "    Define a categorical policy over a discrete action space.\n",
        "    \n",
        "    Does not work on any other action space.\n",
        "    \n",
        "    Inherits from Actor, and uses forward method from Actor.\n",
        "    \n",
        "    Args:\n",
        "        state_dim: Input state size.\n",
        "        hidden_sizes: Hidden layer sizes for MLP.\n",
        "        action_dim: Action space size.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_dim: int,\n",
        "        hidden_sizes: tuple,\n",
        "        action_dim: int\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mlp = MLP(state_dim, hidden_sizes, action_dim)\n",
        "\n",
        "    def action_distribution(self, states):\n",
        "        logits = self.mlp(states)\n",
        "        return torch.distributions.Categorical(logits=logits)\n",
        "\n",
        "    def logprob_from_distribution(self, policy, actions):\n",
        "        return policy.log_prob(actions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbtecYI6_pfh",
        "colab_type": "text"
      },
      "source": [
        "## Preparing for REINFORCE\n",
        "\n",
        "We are going to start with implementing the REINFORCE algorithm.\n",
        "\n",
        "REINFORCE is the simplest policy-gradient algorithm. It needs only one network, the policy, and it learns to get as much reward as possible using only the reward signal from the environment and log-probabilities of actions under the policy. As we begin implementing more algorithm-specific stuff we'll talk about the math.\n",
        "\n",
        "After discussing REINFORCE, we'll talk about an actor-critic method (A2C), which learns a policy (the actor) and a value function (the critic).\n",
        "\n",
        "It's helpful to define a class with a specific method for computing an action and its log-prob at each environment step, so we will do that here. The ReinforceActor has a ```step``` method that we'll call at each timestep of the environment, and it'll return to us the action and log-probability of the selected action."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "colab_type": "code",
        "id": "2GoPRiTEg6FQ",
        "colab": {}
      },
      "source": [
        "class ReinforceActor(nn.Module):\n",
        "    \"\"\"\n",
        "    A cleaned-up actor for the REINFORCE algorithm.\n",
        "    \n",
        "    Args:\n",
        "        state_space: Actual state space from the environment. Is a gym.spaces type.\n",
        "        hidden_sizes: Hidden layer sizes for MLP policy.\n",
        "        action_space: Action space from the environment. Is a gym.spaces type.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_space: gym.spaces,\n",
        "        hidden_sizes: tuple,\n",
        "        action_space: gym.spaces\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        state_size = state_space.shape[0]\n",
        "\n",
        "        \"\"\"\n",
        "        Check to make sure that the action space is compatible with our Categorical Policy.\n",
        "        \"\"\"\n",
        "        if isinstance(action_space, gym.spaces.Discrete):\n",
        "            action_dim = action_space.n\n",
        "            self.policy = CategoricalPolicy(state_size, hidden_sizes, action_dim)\n",
        "        else:\n",
        "            raise ValueError(f\"Env has action space of type {type(action_space)}. REINFORCE Actor currently only supports gym.spaces.Discrete action spaces!\")\n",
        "\n",
        "    def step(self, state):\n",
        "        \"\"\"\n",
        "        Get an action and the log-probability of that action from the policy.\n",
        "        \n",
        "        Args:\n",
        "            state: Current state of the environment.\n",
        "            \n",
        "        Returns:\n",
        "            action: NumPy array of the chosen action.\n",
        "            action_logp: Log prob of the chosen action.\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            policy_current = self.policy.action_distribution(state) # get current policy given current state\n",
        "            action = policy_current.sample() # sample an action from the policy\n",
        "            action_logp = self.policy.logprob_from_distribution(policy_current, action) # calculate the log-probability of that action under the current policy\n",
        "        return action.numpy(), action_logp.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJyVw02Z_pfk",
        "colab_type": "text"
      },
      "source": [
        "## The Policy Gradient Buffer\n",
        "\n",
        "As we train, it's necessary to be able to store the agent's experiences so that we can later use them to compute loss and perform gradient updates. We use the buffer to do this. At each timestep of the environment, we store a tuple of state, action, reward, value (only in actor-critic methods) and logprob of the action. When it is time to calculate loss and update, we need to calculate advantages, normalize returns, and then get everything from the buffer so that we can train on it.\n",
        "\n",
        "### What is Advantage?\n",
        "\n",
        "Advantage can be thought of as an action-value. There are many ways of estimating advantage, but they all aim to estimate the same equation. The advantage is equal to the action-value of a state, action pair minus the state-value of that state:\n",
        "\n",
        "$A(s,a) = Q_\\pi(s,a) - V_\\pi(s)$\n",
        "\n",
        "Intuitively, we can see why this makes sense by recognizing that the Q-value is the value of both being in state $s$ and taking action $a$, while the state-value only estimates the value of state $s$, and disregards any action. By subtracting the pure state-value from the Q-value, we are left with an action-value, AKA Advantage.\n",
        "\n",
        "### But where does Q come from? We are only learning a policy!\n",
        "\n",
        "We have an estimate of Q given to us by the environment; the reward signal! The Q function takes in states and actions and tries to predict how much reward will be earned into the future. The environment gives us this directly. When we calculate advantage, we can substitute Q with the return earned by the agent after being in state $s$ and taking action $a$. Then, we only have to learn a state-value function. Actor-critic  methods learn a value function, but REINFORCE does not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "colab_type": "code",
        "id": "DupvK985eoQm",
        "colab": {}
      },
      "source": [
        "\"\"\"This class is borrowed from OpenAI's SpinningUp code, so thanks to them for this!\"\"\"\n",
        "\n",
        "class PolicyGradientBuffer:\n",
        "    \"\"\"\n",
        "    A buffer for storing trajectories experienced by an agent interacting\n",
        "    with the environment, and using Generalized Advantage Estimation (GAE-Lambda)\n",
        "    for calculating the advantages of state-action pairs.\n",
        "    \n",
        "    Args:\n",
        "        obs_dim: Size of the observations\n",
        "        act_dim: Size of the action space\n",
        "        size: Total size of the buffer. i.e. size=4000 stores 4000 interaction tuples before the buffer is full.\n",
        "        gamma: Discount factor for return calculation\n",
        "        lam: Lambda argument for GAE-lambda advantage estimation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        obs_dim: Union[tuple, int],\n",
        "        act_dim: Union[tuple, int],\n",
        "        size: int,\n",
        "        gamma: Optional[float] = 0.99,\n",
        "        lam: Optional[float] = 0.95,\n",
        "    ):\n",
        "        self.obs_buf = np.zeros(self._combined_shape(size, obs_dim), dtype=np.float32)\n",
        "        self.act_buf = np.zeros(self._combined_shape(size, act_dim), dtype=np.float32)\n",
        "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.val_buf = np.zeros(size, dtype=np.float32) # not used in REINFORCE, as we are not learning a value function\n",
        "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.gamma, self.lam = gamma, lam\n",
        "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size # ptr used to store current location in the buffer, path_start_idx used to mark where episodes begin and end in the buffer\n",
        "\n",
        "    def store(\n",
        "        self,\n",
        "        obs: np.array,\n",
        "        act: np.array,\n",
        "        rew: Union[int, float, np.array],\n",
        "        val: Union[int, float, np.array],\n",
        "        logp: Union[float, np.array],\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Append one timestep of agent-environment interaction to the buffer.\n",
        "        \n",
        "        Args:\n",
        "            obs: Current observations (state)\n",
        "            act: Action taken in the state\n",
        "            rew: Reward earned for taking the action in the current state\n",
        "            val: Estimated value of the current state. NOTE: This is not used in REINFORCE, so pass in zeros for this argument when training REINFORCE.\n",
        "            logp: Log probability of taking the action in the current state.\n",
        "        \"\"\"\n",
        "        assert self.ptr < self.max_size  # buffer has to have room so you can store\n",
        "        self.obs_buf[self.ptr] = obs\n",
        "        self.act_buf[self.ptr] = act\n",
        "        self.rew_buf[self.ptr] = rew\n",
        "        self.val_buf[self.ptr] = val # not used in REINFORCE, because we are not learning a value function. Pass in zeros during REINFORCE training.\n",
        "        self.logp_buf[self.ptr] = logp\n",
        "        self.ptr += 1\n",
        "\n",
        "    def finish_path(self, last_val: Optional[Union[int, float, np.array]] = 0):\n",
        "        \"\"\"\n",
        "        Call this at the end of a trajectory, or when one gets cut off\n",
        "        by an epoch ending. This looks back in the buffer to where the\n",
        "        trajectory started, and uses rewards and value estimates from\n",
        "        the whole trajectory to compute advantage estimates with GAE-Lambda,\n",
        "        as well as compute the rewards-to-go for each state, to use as\n",
        "        the targets for the value function.\n",
        "        The \"last_val\" argument should be 0 if the trajectory ended\n",
        "        because the agent reached a terminal state (died), and otherwise\n",
        "        should be V(s_T), the value function estimated for the last state.\n",
        "        This allows us to bootstrap the reward-to-go calculation to account\n",
        "        for timesteps beyond the arbitrary episode horizon (or epoch cutoff).\n",
        "        \n",
        "        Args:\n",
        "            last_val: This only needs to be passed in when the epoch ends before the episode ends.\n",
        "                When training an actor-critic method, this is the estimate of future reward from the critic. In REINFORCE, pass in the last obtained reward for this argument.\n",
        "        \"\"\"\n",
        "\n",
        "        path_slice = slice(self.path_start_idx, self.ptr)\n",
        "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
        "        vals = np.append(self.val_buf[path_slice], last_val) # not used in REINFORCE, not learning a value function\n",
        "\n",
        "        # the next two lines implement GAE-Lambda advantage calculation\n",
        "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
        "        self.adv_buf[path_slice] = self._discount_cumsum(deltas, self.gamma * self.lam)\n",
        "\n",
        "        # the next line computes rewards-to-go, to be targets for the value function\n",
        "        self.ret_buf[path_slice] = self._discount_cumsum(rews, self.gamma)[:-1]\n",
        "\n",
        "        self.path_start_idx = self.ptr\n",
        "\n",
        "    def get(self):\n",
        "        \"\"\"\n",
        "        Call this at the end of an epoch to get all of the data from\n",
        "        the buffer, with advantages appropriately normalized (shifted to have\n",
        "        mean zero and std one). Also, resets some pointers in the buffer.\n",
        "        \"\"\"\n",
        "        assert self.ptr == self.max_size  # buffer has to be full before you can get\n",
        "        self.ptr, self.path_start_idx = 0, 0\n",
        "        # the next two lines implement the advantage normalization trick\n",
        "        adv_mean, adv_std = np.mean(self.adv_buf), np.std(self.adv_buf)\n",
        "        # adv_mean, adv_std = np.mean(self.adv_buf), np.std(self.adv_buf)\n",
        "        self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
        "        return [self.obs_buf, self.act_buf, self.adv_buf, self.ret_buf, self.logp_buf]\n",
        "\n",
        "    def _combined_shape(\n",
        "        self, length: Union[int, np.array], shape: Optional[Union[int, tuple]] = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Get combined shape of a length and another shape tuple.\n",
        "        \n",
        "        Args:\n",
        "            length: Length to combine into shape tuple\n",
        "            shape: Original shape tuple \n",
        "        \"\"\"\n",
        "        if shape is None:\n",
        "            return (length,)\n",
        "        return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
        "\n",
        "    def _discount_cumsum(self, x: np.array, discount: float):\n",
        "        \"\"\"\n",
        "        magic from rllab for computing discounted cumulative sums of vectors.\n",
        "        input:\n",
        "            vector x,\n",
        "            [x0,\n",
        "            x1,\n",
        "            x2]\n",
        "        output:\n",
        "            [x0 + discount * x1 + discount^2 * x2,\n",
        "            x1 + discount * x2,\n",
        "            x2]\n",
        "            \n",
        "        Args:\n",
        "            x: Vector to discount\n",
        "            discount: discount factor\n",
        "        \"\"\"\n",
        "        return lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Wl0wADj_pfm",
        "colab_type": "text"
      },
      "source": [
        "## Setting up an RL dataset\n",
        "\n",
        "PyTorch Lightning requires that the user feed data to their networks using PyTorch datasets. Here, we set up a simple RL dataset. When initialized, it takes the data from the buffer as input. At each index, it'll return one tuple from the buffer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "colab_type": "code",
        "id": "YOQdbIhDjQif",
        "colab": {}
      },
      "source": [
        "class PolicyGradientRLDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch dataset for training Policy Gradient RL algorithms.\n",
        "    \n",
        "    Args:\n",
        "        data: The result of calling .get() from the buffer.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        data \n",
        "    ):\n",
        "        self.data = data \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data[2]) \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Return idx-th tuple from the buffer.\n",
        "        \n",
        "        Args:\n",
        "            idx: index of the buffer to pull data from.\n",
        "        \n",
        "        Returns:\n",
        "            tuple of:\n",
        "                state: state at that index\n",
        "                action: action taken in state\n",
        "                adv: advantage estimation for taking the action in the state\n",
        "                rew: observed reward for taking the action in the state\n",
        "                logp: log-probability of taking the action in the state\n",
        "        \"\"\"\n",
        "        state = self.data[0][idx]\n",
        "        act = self.data[1][idx]\n",
        "        adv = self.data[2][idx]\n",
        "        rew = self.data[3][idx]\n",
        "        logp = self.data[4][idx]\n",
        "\n",
        "        return state, act, adv, rew, logp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM6Il5fp_pfp",
        "colab_type": "text"
      },
      "source": [
        "## REINFORCE Algorithm\n",
        "\n",
        "Now we'll cover the REINFORCE algorithm implementation. \n",
        "\n",
        "REINFORCE learns to update the policy distribution in such a way that it makes actions which led to high reward more likely and actions that led to low reward less likely. The gradient of our policies performance (performance is denoted by J) under REINFORCE only needs two arguments: the log-probability of a selected action and the return obtained following that action. In a more ML-friendly parlance, we can also call this the policy loss. We denote the weights of our NN policy by $\\theta$. We can write it out:\n",
        "\n",
        "$\\nabla J(\\theta) = \\log{\\mathbb{P} (a_t)} * G_t$\n",
        "\n",
        "This equation is exactly what we need to update our policy weights according to. It's simple to write in code as well:\n",
        "\n",
        "```\n",
        "policy_loss = -(action_logps * returns).mean()\n",
        "```\n",
        "\n",
        "We have to optimize the negative of the loss function instead of the original because pre-built ML optimizers are intended to minimize your loss function. But here, we want to maximize the loss (remember, the loss here is really our policies performance!), so we trick the optimizer into doing this by using the negative loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "colab_type": "code",
        "id": "IUWKabMNbYPx",
        "colab": {}
      },
      "source": [
        "class REINFORCE(pl.LightningModule):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        hparams\n",
        "    ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.hparams = hparams # register hyperparameters to PyTorch Lightning \n",
        "\n",
        "        hids = (int(i) for i in hparams.hidden_layers) # make sure hidden layer sizes is a tuple of ints\n",
        "\n",
        "        torch.manual_seed(hparams.seed) # random seeding\n",
        "        np.random.seed(hparams.seed)\n",
        "\n",
        "        self.env = gym.make(hparams.env_name) # make environment for agent to interact with\n",
        "\n",
        "        self.actor = ReinforceActor(self.env.observation_space, hids, self.env.action_space) # intialize actor\n",
        "\n",
        "        # here we intialize our buffer\n",
        "        self.buffer = PolicyGradientBuffer(\n",
        "            self.env.observation_space.shape[0], # state size\n",
        "            self.env.action_space.shape, # action space size\n",
        "            size=hparams.steps_per_epoch, # how many interactions to collect per epoch\n",
        "            gamma=hparams.gamma, # gamma disount factor for return calculation\n",
        "            lam=hparams.lam # lambda factor for GAE-lambda advantage estimation\n",
        "            )\n",
        "\n",
        "        self.steps_per_epoch = hparams.steps_per_epoch # register variables to class\n",
        "        self.policy_lr = hparams.policy_lr\n",
        "\n",
        "        self.tracker_dict = {} # intialize an empty metrics tracking dictionary\n",
        "\n",
        "        self.inner_loop() # generate our first epoch of data!!!\n",
        "\n",
        "        self.minibatch_size = self.steps_per_epoch // 10 # set minibatch size for dataloader\n",
        "        if hparams.minibatch_size is not None:\n",
        "            self.minibatch_size = hparams.minibatch_size\n",
        "\n",
        "    def forward(self, state: torch.Tensor, a: torch.Tensor = None) -> torch.Tensor:\n",
        "        r\"\"\"\n",
        "        Forward pass for the agent.\n",
        "\n",
        "        Args:\n",
        "            state (PyTorch Tensor): state of the environment\n",
        "            a (PyTorch Tensor): action agent took. Optional. Defaults to None.\n",
        "        \"\"\"\n",
        "        return self.actor.policy(state, a) \n",
        "\n",
        "    def configure_optimizers(self) -> tuple:\n",
        "        r\"\"\"\n",
        "        Set up optimizers for agent.\n",
        "\n",
        "        Returns:\n",
        "            policy_optimizer (torch.optim.Adam): Optimizer for policy network.\n",
        "            value_optimizer (torch.optim.Adam): Optimizer for value network.\n",
        "        \"\"\"\n",
        "        self.policy_optimizer = torch.optim.Adam(self.actor.policy.parameters(), lr=self.policy_lr)\n",
        "        return self.policy_optimizer\n",
        "\n",
        "    def inner_loop(self) -> None:\n",
        "        r\"\"\"\n",
        "        Run agent-env interaction loop. \n",
        "\n",
        "        Stores agent environment interaction tuples to the buffer. Logs reward mean/std/min/max to tracker dict. Collects data at loop end.\n",
        "\n",
        "        \"\"\"\n",
        "        state, reward, episode_reward, episode_length = self.env.reset(), 0, 0, 0 # reset state, reward, etc to initial values \n",
        "        rewlst = [] # empty reward tracking list\n",
        "        lenlst = [] # empty episode length tracking list\n",
        "\n",
        "        for i in range(self.steps_per_epoch): # collect steps_per_epoch interactions between agent and environment\n",
        "            action, logp = self.actor.step(torch.as_tensor(state, dtype=torch.float32)) # get action and action log probability\n",
        "\n",
        "            next_state, reward, done, _ = self.env.step(action) # step environment\n",
        "            \n",
        "            # store interaction\n",
        "            self.buffer.store(\n",
        "                state,\n",
        "                action,\n",
        "                reward,\n",
        "                0, # recall REINFORCE doesn't use a value function, so just pass zero to our buffer for this argument.\n",
        "                logp\n",
        "            )\n",
        "\n",
        "            state = next_state # step the state to the next state\n",
        "            episode_length += 1 # increment episode_length\n",
        "            episode_reward += reward # increment episode reward\n",
        "\n",
        "\n",
        "            timeup = episode_length == 1000 # check if we have hit our max episode length (here, 1000, but other values are valid too)\n",
        "            over = done or timeup # check if episode is over (done == True) or timeup\n",
        "            epoch_ended = i == self.steps_per_epoch - 1 # check if we've hit the end of our epoch\n",
        "            if over or epoch_ended:\n",
        "                if timeup or epoch_ended:\n",
        "                    last_val = reward # if timeup or epoch_ended, the episode was cut off before it truly ended, so give the current reward as an estimate of future reward\n",
        "                else:\n",
        "                    last_val = 0 # otherwise, the episode wasn't cut off before it really ended, and it is unnecessary to estimate future reward\n",
        "                self.buffer.finish_path(last_val) # finish the episode in the buffer\n",
        "\n",
        "                if over:\n",
        "                    # store the episode reward and episode length\n",
        "                    rewlst.append(episode_reward)\n",
        "                    lenlst.append(episode_length)\n",
        "                state, episode_reward, episode_length = self.env.reset(), 0, 0 # reset state, episode_reward, episode_length to intial values\n",
        "\n",
        "        # at epoch end, store epoch metrics in local tracking dict\n",
        "        trackit = {\n",
        "            \"MeanEpReturn\": np.mean(rewlst),\n",
        "            \"StdEpReturn\": np.std(rewlst),\n",
        "            \"MaxEpReturn\": np.max(rewlst),\n",
        "            \"MinEpReturn\": np.min(rewlst),\n",
        "            \"MeanEpLength\": np.mean(lenlst),\n",
        "            \"Epoch\": self.current_epoch\n",
        "        }\n",
        "        self.tracker_dict.update(trackit) # update overall tracking dictionary\n",
        "\n",
        "        self.data = self.buffer.get() # update data with latest epoch data\n",
        "\n",
        "    def calc_pol_loss(self, logps, returns) -> torch.Tensor:\n",
        "        r\"\"\"\n",
        "        Loss for REINFORCE policy gradient agent.\n",
        "        \"\"\"\n",
        "        return -(logps * returns).mean()\n",
        "\n",
        "  \n",
        "    def training_step(self, batch: Tuple, batch_idx: int) -> dict:\n",
        "        r\"\"\"\n",
        "        Calculate policy loss over input batch.\n",
        "\n",
        "        Also compute and log policy entropy and KL divergence.\n",
        "\n",
        "        Args:\n",
        "          batch (Tuple of PyTorch tensors): Batch to train on.\n",
        "          batch_idx: batch index.\n",
        "        \"\"\"\n",
        "        states, acts, _, rets, logps_old = batch\n",
        "\n",
        "        policy, logps = self.actor.policy(states, acts) # get updated policy and logp estimates on stored states and actions (need this for PyTorch gradients)\n",
        "        pol_loss = self.calc_pol_loss(logps, rets)\n",
        "\n",
        "        ent = policy.entropy().mean() # estimate policy entropy\n",
        "        kl = (logps_old - logps).mean() # calculate sample estimate of KL divergence between new and old policy\n",
        "        log = {\"PolicyLoss\": pol_loss, \"Entropy\": ent, \"KL\": kl}\n",
        "        self.tracker_dict.update(log)\n",
        "\n",
        "        return {\"loss\": pol_loss, \"log\": log, \"progress_bar\": log}\n",
        "      \n",
        "    def training_step_end(\n",
        "        self,\n",
        "        step_dict: dict\n",
        "    ) -> dict:\n",
        "        r\"\"\"\n",
        "        Method for end of training step. Makes sure that episode reward and length info get added to logger.\n",
        "\n",
        "        Args:\n",
        "            step_dict (dict): dictioanry from last training step.\n",
        "\n",
        "        Returns:\n",
        "            step_dict (dict): dictionary from last training step with episode return and length info from last epoch added to log.\n",
        "        \"\"\"\n",
        "        step_dict['log'] = self.add_to_log_dict(step_dict['log'])\n",
        "        return step_dict\n",
        "\n",
        "    def add_to_log_dict(self, log_dict) -> dict:\n",
        "        r\"\"\"\n",
        "        Adds episode return and length info to logger dictionary.\n",
        "\n",
        "        Args:\n",
        "            log_dict (dict): Dictionary to log to.\n",
        "\n",
        "        Returns:\n",
        "            log_dict (dict): Modified log_dict to include episode return and length info.\n",
        "        \"\"\"\n",
        "        add_to_dict = {\n",
        "            \"MeanEpReturn\": self.tracker_dict[\"MeanEpReturn\"],\n",
        "            \"MaxEpReturn\": self.tracker_dict[\"MaxEpReturn\"],\n",
        "            \"MinEpReturn\": self.tracker_dict[\"MinEpReturn\"],\n",
        "            \"MeanEpLength\": self.tracker_dict[\"MeanEpLength\"],\n",
        "            }\n",
        "        log_dict.update(add_to_dict)\n",
        "        return log_dict\n",
        "\n",
        "    def train_dataloader(self) -> DataLoader:\n",
        "        r\"\"\"\n",
        "        Define a PyTorch dataset with the data from the last :func:`~inner_loop` run and return a dataloader.\n",
        "\n",
        "        Returns:\n",
        "            dataloader (PyTorch Dataloader): Object for loading data collected during last epoch.\n",
        "        \"\"\"\n",
        "        dataset = PolicyGradientRLDataset(self.data)\n",
        "        dataloader = DataLoader(dataset, batch_size=self.minibatch_size, sampler=None)\n",
        "        return dataloader\n",
        "\n",
        "    def printdict(self, out_file: Optional[str] = sys.stdout) -> None:\n",
        "        r\"\"\"\n",
        "        Print the contents of the epoch tracking dict to stdout or to a file.\n",
        "\n",
        "        Args:\n",
        "            out_file (sys.stdout or string): File for output. If writing to a file, opening it for writing should be handled in :func:`on_epoch_end`.\n",
        "        \"\"\"\n",
        "        self.print(\"\\n\", file=out_file)\n",
        "        for k, v in self.tracker_dict.items():\n",
        "            self.print(f\"{k}: {v}\", file=out_file)\n",
        "        self.print(\"\\n\", file=out_file)\n",
        "  \n",
        "    def on_epoch_end(self) -> None:\n",
        "        r\"\"\"\n",
        "        Print tracker_dict, reset tracker_dict, and generate new data with inner loop.\n",
        "        \"\"\"\n",
        "        self.printdict()\n",
        "        self.tracker_dict = {}\n",
        "        self.inner_loop()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "colab_type": "code",
        "id": "vAUDYLOekb3s",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "91d709cfa9654b11ac90969b25150c81",
            "0cae9ff624fd4f92a0d0afb7a6738406",
            "cbf02aadb0fb4358aa5b89af9f81aa1a",
            "89326d502b334d33b1026d8d90b22dd0",
            "59177c2ad7a64ecc8b3454dad1c5fac6",
            "4336f7b3d62f479ba66ee90eef41257d",
            "e6775ba11c8c4051876da297ee037e61",
            "3b4b77a07d86484197877fc0158ceae4",
            "837d842e7e734081bef50b9d6f79161d"
          ]
        },
        "outputId": "cf386e7c-641a-4682-9b60-cc6a75ecbad9"
      },
      "source": [
        "epochs = 100 # epochs to train for\n",
        "hparams = Namespace(env_name=\"CartPole-v1\", hidden_layers=(64, 32), seed=123, gamma=0.99, lam=0.97, steps_per_epoch=4000, policy_lr=3e-4, minibatch_size=400) # all necessary arguments for REINFORCE. Mess with these!\n",
        "trainer = pl.Trainer(\n",
        "    reload_dataloaders_every_epoch = True, # need to update data every epoch with latest batch of data\n",
        "    early_stop_callback = False, # don't do early stopping\n",
        "    max_epochs = epochs # train for no more than whatever epochs is set to\n",
        ")\n",
        "\n",
        "agent = REINFORCE(hparams) # init agent\n",
        "trainer.fit(agent) # run training"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "\n",
            "  | Name  | Type           | Params\n",
            "-----------------------------------------\n",
            "0 | actor | ReinforceActor | 2 K   \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "837d842e7e734081bef50b9d6f79161d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "MeanEpReturn: 21.524324324324326\n",
            "StdEpReturn: 9.622647986557135\n",
            "MaxEpReturn: 62.0\n",
            "MinEpReturn: 8.0\n",
            "MeanEpLength: 21.524324324324326\n",
            "Epoch: 0\n",
            "PolicyLoss: 8.836243629455566\n",
            "Entropy: 0.6915397644042969\n",
            "KL: -0.002231425838544965\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 24.968553459119498\n",
            "StdEpReturn: 12.326492093920384\n",
            "MaxEpReturn: 80.0\n",
            "MinEpReturn: 9.0\n",
            "MeanEpLength: 24.968553459119498\n",
            "Epoch: 1\n",
            "PolicyLoss: 10.299190521240234\n",
            "Entropy: 0.6858498454093933\n",
            "KL: 0.0006316952640190721\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 26.526666666666667\n",
            "StdEpReturn: 12.848707673882572\n",
            "MaxEpReturn: 88.0\n",
            "MinEpReturn: 10.0\n",
            "MeanEpLength: 26.526666666666667\n",
            "Epoch: 2\n",
            "PolicyLoss: 9.17562484741211\n",
            "Entropy: 0.6789462566375732\n",
            "KL: 0.0024496272671967745\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 28.78985507246377\n",
            "StdEpReturn: 14.92126820704367\n",
            "MaxEpReturn: 94.0\n",
            "MinEpReturn: 9.0\n",
            "MeanEpLength: 28.78985507246377\n",
            "Epoch: 3\n",
            "PolicyLoss: 9.161133766174316\n",
            "Entropy: 0.6736569404602051\n",
            "KL: 6.354674405883998e-05\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 29.375\n",
            "StdEpReturn: 16.10714926214261\n",
            "MaxEpReturn: 95.0\n",
            "MinEpReturn: 10.0\n",
            "MeanEpLength: 29.375\n",
            "Epoch: 4\n",
            "PolicyLoss: 9.790409088134766\n",
            "Entropy: 0.6674461364746094\n",
            "KL: 0.0013573822798207402\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 34.4051724137931\n",
            "StdEpReturn: 18.740964925787143\n",
            "MaxEpReturn: 112.0\n",
            "MinEpReturn: 10.0\n",
            "MeanEpLength: 34.4051724137931\n",
            "Epoch: 5\n",
            "PolicyLoss: 13.433465957641602\n",
            "Entropy: 0.6615338325500488\n",
            "KL: 0.000973659916780889\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 34.87719298245614\n",
            "StdEpReturn: 20.24288497523642\n",
            "MaxEpReturn: 114.0\n",
            "MinEpReturn: 11.0\n",
            "MeanEpLength: 34.87719298245614\n",
            "Epoch: 6\n",
            "PolicyLoss: 10.368790626525879\n",
            "Entropy: 0.6551957726478577\n",
            "KL: -0.0006159919430501759\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 41.94736842105263\n",
            "StdEpReturn: 22.818906482888096\n",
            "MaxEpReturn: 122.0\n",
            "MinEpReturn: 11.0\n",
            "MeanEpLength: 41.94736842105263\n",
            "Epoch: 7\n",
            "PolicyLoss: 12.8630952835083\n",
            "Entropy: 0.647998571395874\n",
            "KL: 0.0002611745148897171\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 40.303030303030305\n",
            "StdEpReturn: 22.0854422299856\n",
            "MaxEpReturn: 114.0\n",
            "MinEpReturn: 12.0\n",
            "MeanEpLength: 40.303030303030305\n",
            "Epoch: 8\n",
            "PolicyLoss: 14.069637298583984\n",
            "Entropy: 0.6484934091567993\n",
            "KL: -0.0014396425103768706\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 45.770114942528735\n",
            "StdEpReturn: 23.069460904485172\n",
            "MaxEpReturn: 135.0\n",
            "MinEpReturn: 12.0\n",
            "MeanEpLength: 45.770114942528735\n",
            "Epoch: 9\n",
            "PolicyLoss: 13.766231536865234\n",
            "Entropy: 0.6369503140449524\n",
            "KL: 0.0016464039217680693\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 48.1566265060241\n",
            "StdEpReturn: 30.190751766625425\n",
            "MaxEpReturn: 226.0\n",
            "MinEpReturn: 16.0\n",
            "MeanEpLength: 48.1566265060241\n",
            "Epoch: 10\n",
            "PolicyLoss: 11.489563941955566\n",
            "Entropy: 0.6172984838485718\n",
            "KL: 0.0012262507807463408\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 55.44927536231884\n",
            "StdEpReturn: 31.984840717233286\n",
            "MaxEpReturn: 158.0\n",
            "MinEpReturn: 16.0\n",
            "MeanEpLength: 55.44927536231884\n",
            "Epoch: 11\n",
            "PolicyLoss: 21.89959144592285\n",
            "Entropy: 0.6209217309951782\n",
            "KL: -0.00010675698285922408\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 59.59090909090909\n",
            "StdEpReturn: 28.475389306263235\n",
            "MaxEpReturn: 147.0\n",
            "MinEpReturn: 20.0\n",
            "MeanEpLength: 59.59090909090909\n",
            "Epoch: 12\n",
            "PolicyLoss: 14.553918838500977\n",
            "Entropy: 0.6237980127334595\n",
            "KL: 0.0006953837582841516\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 56.014084507042256\n",
            "StdEpReturn: 26.521579286004904\n",
            "MaxEpReturn: 126.0\n",
            "MinEpReturn: 15.0\n",
            "MeanEpLength: 56.014084507042256\n",
            "Epoch: 13\n",
            "PolicyLoss: 14.128946304321289\n",
            "Entropy: 0.612790048122406\n",
            "KL: 0.0025930542033165693\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 67.52542372881356\n",
            "StdEpReturn: 33.806706752220386\n",
            "MaxEpReturn: 166.0\n",
            "MinEpReturn: 18.0\n",
            "MeanEpLength: 67.52542372881356\n",
            "Epoch: 14\n",
            "PolicyLoss: 16.656187057495117\n",
            "Entropy: 0.6326173543930054\n",
            "KL: 0.0009899308206513524\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 58.794117647058826\n",
            "StdEpReturn: 34.63403580274986\n",
            "MaxEpReturn: 210.0\n",
            "MinEpReturn: 16.0\n",
            "MeanEpLength: 58.794117647058826\n",
            "Epoch: 15\n",
            "PolicyLoss: 13.360077857971191\n",
            "Entropy: 0.5865457653999329\n",
            "KL: 0.005039578303694725\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 78.13725490196079\n",
            "StdEpReturn: 45.45155559703019\n",
            "MaxEpReturn: 278.0\n",
            "MinEpReturn: 23.0\n",
            "MeanEpLength: 78.13725490196079\n",
            "Epoch: 16\n",
            "PolicyLoss: 19.928363800048828\n",
            "Entropy: 0.6053346395492554\n",
            "KL: 0.0017995344242081046\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 83.0\n",
            "StdEpReturn: 43.29020289472742\n",
            "MaxEpReturn: 201.0\n",
            "MinEpReturn: 19.0\n",
            "MeanEpLength: 83.0\n",
            "Epoch: 17\n",
            "PolicyLoss: 21.665729522705078\n",
            "Entropy: 0.6149697303771973\n",
            "KL: 0.003909469582140446\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 87.37777777777778\n",
            "StdEpReturn: 43.809328732024845\n",
            "MaxEpReturn: 208.0\n",
            "MinEpReturn: 25.0\n",
            "MeanEpLength: 87.37777777777778\n",
            "Epoch: 18\n",
            "PolicyLoss: 18.50716209411621\n",
            "Entropy: 0.6042423844337463\n",
            "KL: 0.004144852980971336\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 96.63414634146342\n",
            "StdEpReturn: 51.77566812352228\n",
            "MaxEpReturn: 240.0\n",
            "MinEpReturn: 25.0\n",
            "MeanEpLength: 96.63414634146342\n",
            "Epoch: 19\n",
            "PolicyLoss: 19.4796085357666\n",
            "Entropy: 0.6029363870620728\n",
            "KL: 0.0038633004296571016\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 165.33333333333334\n",
            "StdEpReturn: 60.4253442044166\n",
            "MaxEpReturn: 283.0\n",
            "MinEpReturn: 57.0\n",
            "MeanEpLength: 165.33333333333334\n",
            "Epoch: 20\n",
            "PolicyLoss: 24.900556564331055\n",
            "Entropy: 0.6112808585166931\n",
            "KL: -0.0015310888411477208\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 137.3448275862069\n",
            "StdEpReturn: 66.42614559097737\n",
            "MaxEpReturn: 293.0\n",
            "MinEpReturn: 34.0\n",
            "MeanEpLength: 137.3448275862069\n",
            "Epoch: 21\n",
            "PolicyLoss: 32.897911071777344\n",
            "Entropy: 0.6070713996887207\n",
            "KL: 0.00022094658925198019\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 159.52\n",
            "StdEpReturn: 84.18604159835525\n",
            "MaxEpReturn: 368.0\n",
            "MinEpReturn: 37.0\n",
            "MeanEpLength: 159.52\n",
            "Epoch: 22\n",
            "PolicyLoss: 31.358610153198242\n",
            "Entropy: 0.6009460687637329\n",
            "KL: -0.00013887621753383428\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 154.8\n",
            "StdEpReturn: 77.50509660661034\n",
            "MaxEpReturn: 364.0\n",
            "MinEpReturn: 39.0\n",
            "MeanEpLength: 154.8\n",
            "Epoch: 23\n",
            "PolicyLoss: 33.917335510253906\n",
            "Entropy: 0.6092144846916199\n",
            "KL: -0.0011469278251752257\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 171.69565217391303\n",
            "StdEpReturn: 83.62828457583241\n",
            "MaxEpReturn: 321.0\n",
            "MinEpReturn: 28.0\n",
            "MeanEpLength: 171.69565217391303\n",
            "Epoch: 24\n",
            "PolicyLoss: 29.739501953125\n",
            "Entropy: 0.598629891872406\n",
            "KL: 0.0007172523182816803\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 158.56\n",
            "StdEpReturn: 58.77555954646455\n",
            "MaxEpReturn: 297.0\n",
            "MinEpReturn: 33.0\n",
            "MeanEpLength: 158.56\n",
            "Epoch: 25\n",
            "PolicyLoss: 30.122716903686523\n",
            "Entropy: 0.5893076062202454\n",
            "KL: -0.00012328647426329553\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 162.91304347826087\n",
            "StdEpReturn: 69.02168093116342\n",
            "MaxEpReturn: 316.0\n",
            "MinEpReturn: 47.0\n",
            "MeanEpLength: 162.91304347826087\n",
            "Epoch: 26\n",
            "PolicyLoss: 33.77403259277344\n",
            "Entropy: 0.6105044484138489\n",
            "KL: -0.0006758149829693139\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 189.2\n",
            "StdEpReturn: 70.78813459895663\n",
            "MaxEpReturn: 316.0\n",
            "MinEpReturn: 22.0\n",
            "MeanEpLength: 189.2\n",
            "Epoch: 27\n",
            "PolicyLoss: 33.48646545410156\n",
            "Entropy: 0.6061395406723022\n",
            "KL: 0.00018877096590586007\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 177.3181818181818\n",
            "StdEpReturn: 58.95868072691121\n",
            "MaxEpReturn: 314.0\n",
            "MinEpReturn: 65.0\n",
            "MeanEpLength: 177.3181818181818\n",
            "Epoch: 28\n",
            "PolicyLoss: 27.003705978393555\n",
            "Entropy: 0.6004080772399902\n",
            "KL: -0.0005431537865661085\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 224.94117647058823\n",
            "StdEpReturn: 79.9672510823946\n",
            "MaxEpReturn: 389.0\n",
            "MinEpReturn: 93.0\n",
            "MeanEpLength: 224.94117647058823\n",
            "Epoch: 29\n",
            "PolicyLoss: 34.17381286621094\n",
            "Entropy: 0.581746518611908\n",
            "KL: -1.3511851648217998e-05\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 216.77777777777777\n",
            "StdEpReturn: 65.06966826713551\n",
            "MaxEpReturn: 338.0\n",
            "MinEpReturn: 114.0\n",
            "MeanEpLength: 216.77777777777777\n",
            "Epoch: 30\n",
            "PolicyLoss: 26.746068954467773\n",
            "Entropy: 0.5869140625\n",
            "KL: 0.00166627939324826\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 210.21052631578948\n",
            "StdEpReturn: 105.04714678224923\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 86.0\n",
            "MeanEpLength: 210.21052631578948\n",
            "Epoch: 31\n",
            "PolicyLoss: 30.053518295288086\n",
            "Entropy: 0.585102915763855\n",
            "KL: -7.81019771238789e-05\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 187.0952380952381\n",
            "StdEpReturn: 88.04214616383331\n",
            "MaxEpReturn: 408.0\n",
            "MinEpReturn: 26.0\n",
            "MeanEpLength: 187.0952380952381\n",
            "Epoch: 32\n",
            "PolicyLoss: 23.11638641357422\n",
            "Entropy: 0.5794314742088318\n",
            "KL: 0.000249629927566275\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 231.64705882352942\n",
            "StdEpReturn: 121.16636165715232\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 74.0\n",
            "MeanEpLength: 231.64705882352942\n",
            "Epoch: 33\n",
            "PolicyLoss: 35.95254135131836\n",
            "Entropy: 0.5565890669822693\n",
            "KL: 0.002082091523334384\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 239.4375\n",
            "StdEpReturn: 94.2224553583168\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 114.0\n",
            "MeanEpLength: 239.4375\n",
            "Epoch: 34\n",
            "PolicyLoss: 32.962955474853516\n",
            "Entropy: 0.5791523456573486\n",
            "KL: 0.0007948355632834136\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 243.8125\n",
            "StdEpReturn: 88.13144923209876\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 129.0\n",
            "MeanEpLength: 243.8125\n",
            "Epoch: 35\n",
            "PolicyLoss: 27.841747283935547\n",
            "Entropy: 0.5514258742332458\n",
            "KL: 0.0009004157618619502\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 265.6666666666667\n",
            "StdEpReturn: 108.65460055709661\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 99.0\n",
            "MeanEpLength: 265.6666666666667\n",
            "Epoch: 36\n",
            "PolicyLoss: 38.38532638549805\n",
            "Entropy: 0.5502049922943115\n",
            "KL: 0.00015778988017700613\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 264.8\n",
            "StdEpReturn: 124.14196174809977\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 99.0\n",
            "MeanEpLength: 264.8\n",
            "Epoch: 37\n",
            "PolicyLoss: 40.6127815246582\n",
            "Entropy: 0.562150239944458\n",
            "KL: 0.0005444450653158128\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 247.3125\n",
            "StdEpReturn: 108.79666742943003\n",
            "MaxEpReturn: 482.0\n",
            "MinEpReturn: 42.0\n",
            "MeanEpLength: 247.3125\n",
            "Epoch: 38\n",
            "PolicyLoss: 31.285133361816406\n",
            "Entropy: 0.5768507719039917\n",
            "KL: 0.00031007343204692006\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 320.3333333333333\n",
            "StdEpReturn: 121.95103753373957\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 129.0\n",
            "MeanEpLength: 320.3333333333333\n",
            "Epoch: 39\n",
            "PolicyLoss: 32.91950607299805\n",
            "Entropy: 0.5824607014656067\n",
            "KL: -0.0002727992832660675\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 329.75\n",
            "StdEpReturn: 130.68035111165975\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 165.0\n",
            "MeanEpLength: 329.75\n",
            "Epoch: 40\n",
            "PolicyLoss: 28.564077377319336\n",
            "Entropy: 0.5740859508514404\n",
            "KL: 0.001257769763469696\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 327.25\n",
            "StdEpReturn: 121.53677427017718\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 141.0\n",
            "MeanEpLength: 327.25\n",
            "Epoch: 41\n",
            "PolicyLoss: 33.98176574707031\n",
            "Entropy: 0.5857961177825928\n",
            "KL: 0.00041104896808974445\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 319.3333333333333\n",
            "StdEpReturn: 107.95009134883685\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 103.0\n",
            "MeanEpLength: 319.3333333333333\n",
            "Epoch: 42\n",
            "PolicyLoss: 31.8402042388916\n",
            "Entropy: 0.5876082181930542\n",
            "KL: 0.0018070742953568697\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 271.2857142857143\n",
            "StdEpReturn: 99.46171450895679\n",
            "MaxEpReturn: 420.0\n",
            "MinEpReturn: 85.0\n",
            "MeanEpLength: 271.2857142857143\n",
            "Epoch: 43\n",
            "PolicyLoss: 33.38642501831055\n",
            "Entropy: 0.5562534928321838\n",
            "KL: 0.00038964330451563\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 300.25\n",
            "StdEpReturn: 109.4593722194069\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 124.0\n",
            "MeanEpLength: 300.25\n",
            "Epoch: 44\n",
            "PolicyLoss: 42.40626525878906\n",
            "Entropy: 0.569720447063446\n",
            "KL: 0.00028610220761038363\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 340.90909090909093\n",
            "StdEpReturn: 144.90652444402315\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 109.0\n",
            "MeanEpLength: 340.90909090909093\n",
            "Epoch: 45\n",
            "PolicyLoss: 33.385501861572266\n",
            "Entropy: 0.567019522190094\n",
            "KL: -8.626282124168938e-07\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 296.7692307692308\n",
            "StdEpReturn: 75.72232050383299\n",
            "MaxEpReturn: 425.0\n",
            "MinEpReturn: 187.0\n",
            "MeanEpLength: 296.7692307692308\n",
            "Epoch: 46\n",
            "PolicyLoss: 33.516754150390625\n",
            "Entropy: 0.5563071370124817\n",
            "KL: -0.0012437424156814814\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 314.8333333333333\n",
            "StdEpReturn: 73.98629753016583\n",
            "MaxEpReturn: 424.0\n",
            "MinEpReturn: 179.0\n",
            "MeanEpLength: 314.8333333333333\n",
            "Epoch: 47\n",
            "PolicyLoss: 32.14553451538086\n",
            "Entropy: 0.5626289248466492\n",
            "KL: 0.000968376116361469\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 336.72727272727275\n",
            "StdEpReturn: 89.01081743156116\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 155.0\n",
            "MeanEpLength: 336.72727272727275\n",
            "Epoch: 48\n",
            "PolicyLoss: 35.734596252441406\n",
            "Entropy: 0.5514227747917175\n",
            "KL: 0.0013002821942791343\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 313.4166666666667\n",
            "StdEpReturn: 104.48720681925079\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 125.0\n",
            "MeanEpLength: 313.4166666666667\n",
            "Epoch: 49\n",
            "PolicyLoss: 31.51373291015625\n",
            "Entropy: 0.5686174035072327\n",
            "KL: -0.00029923408874310553\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 261.1333333333333\n",
            "StdEpReturn: 99.53683851832056\n",
            "MaxEpReturn: 470.0\n",
            "MinEpReturn: 126.0\n",
            "MeanEpLength: 261.1333333333333\n",
            "Epoch: 50\n",
            "PolicyLoss: 23.631610870361328\n",
            "Entropy: 0.5334765315055847\n",
            "KL: 0.000914482050575316\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 271.7857142857143\n",
            "StdEpReturn: 112.5942009236384\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 114.0\n",
            "MeanEpLength: 271.7857142857143\n",
            "Epoch: 51\n",
            "PolicyLoss: 32.67664337158203\n",
            "Entropy: 0.5546078681945801\n",
            "KL: 7.35800713300705e-05\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 397.5\n",
            "StdEpReturn: 116.4390398448905\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 213.0\n",
            "MeanEpLength: 397.5\n",
            "Epoch: 52\n",
            "PolicyLoss: 38.41740417480469\n",
            "Entropy: 0.5622664093971252\n",
            "KL: -0.00033862711279653013\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 319.1818181818182\n",
            "StdEpReturn: 114.89981262886707\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 112.0\n",
            "MeanEpLength: 319.1818181818182\n",
            "Epoch: 53\n",
            "PolicyLoss: 41.70391082763672\n",
            "Entropy: 0.5578431487083435\n",
            "KL: -0.0009401054121553898\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 415.77777777777777\n",
            "StdEpReturn: 81.48043770375232\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 272.0\n",
            "MeanEpLength: 415.77777777777777\n",
            "Epoch: 54\n",
            "PolicyLoss: 31.720335006713867\n",
            "Entropy: 0.5655512809753418\n",
            "KL: -0.00039426633156836033\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 399.77777777777777\n",
            "StdEpReturn: 151.86454328174446\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 114.0\n",
            "MeanEpLength: 399.77777777777777\n",
            "Epoch: 55\n",
            "PolicyLoss: 40.02804183959961\n",
            "Entropy: 0.5645250678062439\n",
            "KL: -0.0002589196083135903\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 397.0\n",
            "StdEpReturn: 89.64621823838664\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 263.0\n",
            "MeanEpLength: 397.0\n",
            "Epoch: 56\n",
            "PolicyLoss: 41.922298431396484\n",
            "Entropy: 0.5509124994277954\n",
            "KL: 0.0001348421792499721\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 380.4\n",
            "StdEpReturn: 119.36264072145856\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 155.0\n",
            "MeanEpLength: 380.4\n",
            "Epoch: 57\n",
            "PolicyLoss: 31.264249801635742\n",
            "Entropy: 0.5586200952529907\n",
            "KL: -0.0004857078311033547\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 396.3\n",
            "StdEpReturn: 150.86023332873378\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 46.0\n",
            "MeanEpLength: 396.3\n",
            "Epoch: 58\n",
            "PolicyLoss: 38.059425354003906\n",
            "Entropy: 0.5467851758003235\n",
            "KL: -0.0009405061136931181\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 337.72727272727275\n",
            "StdEpReturn: 153.7607887768831\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 121.0\n",
            "MeanEpLength: 337.72727272727275\n",
            "Epoch: 59\n",
            "PolicyLoss: 31.83289337158203\n",
            "Entropy: 0.5564187169075012\n",
            "KL: -0.0004296097904443741\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 401.22222222222223\n",
            "StdEpReturn: 76.07566084392239\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 279.0\n",
            "MeanEpLength: 401.22222222222223\n",
            "Epoch: 60\n",
            "PolicyLoss: 43.322994232177734\n",
            "Entropy: 0.527366042137146\n",
            "KL: 0.0017353120492771268\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 325.1666666666667\n",
            "StdEpReturn: 120.801375084153\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 137.0\n",
            "MeanEpLength: 325.1666666666667\n",
            "Epoch: 61\n",
            "PolicyLoss: 28.11661148071289\n",
            "Entropy: 0.5410764217376709\n",
            "KL: -0.0018205909291282296\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 321.3333333333333\n",
            "StdEpReturn: 105.24917524089625\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 181.0\n",
            "MeanEpLength: 321.3333333333333\n",
            "Epoch: 62\n",
            "PolicyLoss: 31.923690795898438\n",
            "Entropy: 0.5307669639587402\n",
            "KL: 0.00041795038850978017\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 366.0\n",
            "StdEpReturn: 127.7614965472775\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 171.0\n",
            "MeanEpLength: 366.0\n",
            "Epoch: 63\n",
            "PolicyLoss: 35.3609733581543\n",
            "Entropy: 0.5310229659080505\n",
            "KL: 0.0004369926464278251\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 357.1818181818182\n",
            "StdEpReturn: 128.78086968449242\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 141.0\n",
            "MeanEpLength: 357.1818181818182\n",
            "Epoch: 64\n",
            "PolicyLoss: 33.521812438964844\n",
            "Entropy: 0.5472110509872437\n",
            "KL: 0.0010718757985159755\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 365.3\n",
            "StdEpReturn: 144.00489575010982\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 122.0\n",
            "MeanEpLength: 365.3\n",
            "Epoch: 65\n",
            "PolicyLoss: 36.90013885498047\n",
            "Entropy: 0.5226331949234009\n",
            "KL: -2.3606418835697696e-05\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 325.25\n",
            "StdEpReturn: 85.9594138726721\n",
            "MaxEpReturn: 490.0\n",
            "MinEpReturn: 118.0\n",
            "MeanEpLength: 325.25\n",
            "Epoch: 66\n",
            "PolicyLoss: 29.823976516723633\n",
            "Entropy: 0.547197163105011\n",
            "KL: 0.002908012829720974\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 377.2\n",
            "StdEpReturn: 128.21996724379554\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 130.0\n",
            "MeanEpLength: 377.2\n",
            "Epoch: 67\n",
            "PolicyLoss: 31.359315872192383\n",
            "Entropy: 0.5398715734481812\n",
            "KL: -0.0033064966555684805\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 423.6666666666667\n",
            "StdEpReturn: 79.44390054203868\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 273.0\n",
            "MeanEpLength: 423.6666666666667\n",
            "Epoch: 68\n",
            "PolicyLoss: 31.83361053466797\n",
            "Entropy: 0.517143189907074\n",
            "KL: -0.0009624040103517473\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 396.22222222222223\n",
            "StdEpReturn: 124.09206060894\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 178.0\n",
            "MeanEpLength: 396.22222222222223\n",
            "Epoch: 69\n",
            "PolicyLoss: 41.63760757446289\n",
            "Entropy: 0.5319832563400269\n",
            "KL: 0.000546522787772119\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 345.3636363636364\n",
            "StdEpReturn: 137.37226974854775\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 166.0\n",
            "MeanEpLength: 345.3636363636364\n",
            "Epoch: 70\n",
            "PolicyLoss: 31.599470138549805\n",
            "Entropy: 0.5371807217597961\n",
            "KL: 0.0003094450512435287\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 332.25\n",
            "StdEpReturn: 121.24639169888727\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 141.0\n",
            "MeanEpLength: 332.25\n",
            "Epoch: 71\n",
            "PolicyLoss: 28.711599349975586\n",
            "Entropy: 0.5434074997901917\n",
            "KL: 0.001219003926962614\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 438.55555555555554\n",
            "StdEpReturn: 88.50374896153785\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 241.0\n",
            "MeanEpLength: 438.55555555555554\n",
            "Epoch: 72\n",
            "PolicyLoss: 34.89900207519531\n",
            "Entropy: 0.5325606465339661\n",
            "KL: 0.0004679913108702749\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 386.3\n",
            "StdEpReturn: 134.24235546205227\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 163.0\n",
            "MeanEpLength: 386.3\n",
            "Epoch: 73\n",
            "PolicyLoss: 30.54619789123535\n",
            "Entropy: 0.5505850315093994\n",
            "KL: 0.0004124983388464898\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 358.90909090909093\n",
            "StdEpReturn: 132.85019214784376\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 128.0\n",
            "MeanEpLength: 358.90909090909093\n",
            "Epoch: 74\n",
            "PolicyLoss: 34.40937805175781\n",
            "Entropy: 0.5476679801940918\n",
            "KL: -0.0006471505621448159\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 430.22222222222223\n",
            "StdEpReturn: 66.18287421611555\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 331.0\n",
            "MeanEpLength: 430.22222222222223\n",
            "Epoch: 75\n",
            "PolicyLoss: 32.88336181640625\n",
            "Entropy: 0.5280259847640991\n",
            "KL: 0.0007353039691224694\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 386.5\n",
            "StdEpReturn: 121.59872532226643\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 182.0\n",
            "MeanEpLength: 386.5\n",
            "Epoch: 76\n",
            "PolicyLoss: 32.03731918334961\n",
            "Entropy: 0.5456985235214233\n",
            "KL: -2.4397448214585893e-05\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 381.7\n",
            "StdEpReturn: 145.82801514112438\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 125.0\n",
            "MeanEpLength: 381.7\n",
            "Epoch: 77\n",
            "PolicyLoss: 30.756248474121094\n",
            "Entropy: 0.5613324642181396\n",
            "KL: -0.00018154800636693835\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 443.5\n",
            "StdEpReturn: 52.76125472351847\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 355.0\n",
            "MeanEpLength: 443.5\n",
            "Epoch: 78\n",
            "PolicyLoss: 41.055519104003906\n",
            "Entropy: 0.5399509072303772\n",
            "KL: -0.00013385355123318732\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 444.625\n",
            "StdEpReturn: 112.73745772812158\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 160.0\n",
            "MeanEpLength: 444.625\n",
            "Epoch: 79\n",
            "PolicyLoss: 41.10257339477539\n",
            "Entropy: 0.5536044836044312\n",
            "KL: 0.001073649269528687\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 370.6\n",
            "StdEpReturn: 129.18142281303454\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 91.0\n",
            "MeanEpLength: 370.6\n",
            "Epoch: 80\n",
            "PolicyLoss: 33.40681457519531\n",
            "Entropy: 0.5333796143531799\n",
            "KL: 0.0008783177472651005\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 406.77777777777777\n",
            "StdEpReturn: 120.81830046973457\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 167.0\n",
            "MeanEpLength: 406.77777777777777\n",
            "Epoch: 81\n",
            "PolicyLoss: 34.497337341308594\n",
            "Entropy: 0.5472047924995422\n",
            "KL: -0.000785678275860846\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 421.77777777777777\n",
            "StdEpReturn: 94.20400767339144\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 270.0\n",
            "MeanEpLength: 421.77777777777777\n",
            "Epoch: 82\n",
            "PolicyLoss: 29.82802963256836\n",
            "Entropy: 0.5540211796760559\n",
            "KL: -0.0003469132643658668\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 392.0\n",
            "StdEpReturn: 114.6172374083022\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 145.0\n",
            "MeanEpLength: 392.0\n",
            "Epoch: 83\n",
            "PolicyLoss: 42.105098724365234\n",
            "Entropy: 0.5264260172843933\n",
            "KL: 0.00038044131360948086\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 460.5\n",
            "StdEpReturn: 84.51922858142991\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 241.0\n",
            "MeanEpLength: 460.5\n",
            "Epoch: 84\n",
            "PolicyLoss: 32.528175354003906\n",
            "Entropy: 0.5429803729057312\n",
            "KL: 0.0005818031495437026\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 437.3333333333333\n",
            "StdEpReturn: 89.90859555991048\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 221.0\n",
            "MeanEpLength: 437.3333333333333\n",
            "Epoch: 85\n",
            "PolicyLoss: 33.984466552734375\n",
            "Entropy: 0.5649820566177368\n",
            "KL: 0.0005356965702958405\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 461.375\n",
            "StdEpReturn: 68.52360450968702\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 303.0\n",
            "MeanEpLength: 461.375\n",
            "Epoch: 86\n",
            "PolicyLoss: 32.60087585449219\n",
            "Entropy: 0.5355055332183838\n",
            "KL: 0.0004911888972856104\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 380.2\n",
            "StdEpReturn: 120.36594202680425\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 143.0\n",
            "MeanEpLength: 380.2\n",
            "Epoch: 87\n",
            "PolicyLoss: 30.589183807373047\n",
            "Entropy: 0.5316169261932373\n",
            "KL: -0.0011723601492121816\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 445.625\n",
            "StdEpReturn: 108.4388508561392\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 175.0\n",
            "MeanEpLength: 445.625\n",
            "Epoch: 88\n",
            "PolicyLoss: 41.14861297607422\n",
            "Entropy: 0.5401625633239746\n",
            "KL: 0.0008552537183277309\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 408.1111111111111\n",
            "StdEpReturn: 107.00513429472485\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 233.0\n",
            "MeanEpLength: 408.1111111111111\n",
            "Epoch: 89\n",
            "PolicyLoss: 32.85787582397461\n",
            "Entropy: 0.5375134944915771\n",
            "KL: -0.0002336329926038161\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 450.25\n",
            "StdEpReturn: 119.64191364233523\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 135.0\n",
            "MeanEpLength: 450.25\n",
            "Epoch: 90\n",
            "PolicyLoss: 39.00591278076172\n",
            "Entropy: 0.5426720976829529\n",
            "KL: 0.0008420042577199638\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 464.25\n",
            "StdEpReturn: 46.68176838981145\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 389.0\n",
            "MeanEpLength: 464.25\n",
            "Epoch: 91\n",
            "PolicyLoss: 31.147485733032227\n",
            "Entropy: 0.5463693141937256\n",
            "KL: 0.001361930393613875\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 500.0\n",
            "StdEpReturn: 0.0\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 500.0\n",
            "MeanEpLength: 500.0\n",
            "Epoch: 92\n",
            "PolicyLoss: 40.624420166015625\n",
            "Entropy: 0.5333375930786133\n",
            "KL: -0.0012985649518668652\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 474.875\n",
            "StdEpReturn: 44.82587840745566\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 378.0\n",
            "MeanEpLength: 474.875\n",
            "Epoch: 93\n",
            "PolicyLoss: 30.785104751586914\n",
            "Entropy: 0.5370449423789978\n",
            "KL: 0.0010500780772417784\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 484.75\n",
            "StdEpReturn: 40.34770749373501\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 378.0\n",
            "MeanEpLength: 484.75\n",
            "Epoch: 94\n",
            "PolicyLoss: 31.880117416381836\n",
            "Entropy: 0.5263677835464478\n",
            "KL: 0.0012767501175403595\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 442.55555555555554\n",
            "StdEpReturn: 110.83432191149204\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 184.0\n",
            "MeanEpLength: 442.55555555555554\n",
            "Epoch: 95\n",
            "PolicyLoss: 39.01247787475586\n",
            "Entropy: 0.529666006565094\n",
            "KL: 0.00016711867647245526\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 438.0\n",
            "StdEpReturn: 103.38600807975259\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 187.0\n",
            "MeanEpLength: 438.0\n",
            "Epoch: 96\n",
            "PolicyLoss: 34.25303268432617\n",
            "Entropy: 0.5323514342308044\n",
            "KL: 0.00015727475692983717\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 464.625\n",
            "StdEpReturn: 93.59345262890989\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 217.0\n",
            "MeanEpLength: 464.625\n",
            "Epoch: 97\n",
            "PolicyLoss: 31.91547393798828\n",
            "Entropy: 0.5242354869842529\n",
            "KL: 0.0014136211248114705\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 437.55555555555554\n",
            "StdEpReturn: 116.84663368051788\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 214.0\n",
            "MeanEpLength: 437.55555555555554\n",
            "Epoch: 98\n",
            "PolicyLoss: 34.42934036254883\n",
            "Entropy: 0.5272660255432129\n",
            "KL: 0.0008030962198972702\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 390.1\n",
            "StdEpReturn: 127.02003778931889\n",
            "MaxEpReturn: 500.0\n",
            "MinEpReturn: 210.0\n",
            "MeanEpLength: 390.1\n",
            "Epoch: 99\n",
            "PolicyLoss: 27.37238311767578\n",
            "Entropy: 0.5039919018745422\n",
            "KL: 0.0006960199098102748\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wh8zW1Am_pfu",
        "colab_type": "text"
      },
      "source": [
        "## The Actor-Critic\n",
        "\n",
        "While the Actor-Critic is largely similar to the REINFORCE Actor, it is different in one key way: it also contains a value function. The value function is trained to estimate future return from the current state. We train it in a supervised fashion. \n",
        "- We store the observed returns from the environment during the interaction period.\n",
        "- Then, during the training step, we update the value function using Mean Squared Error between the observed return and predicted return.  \n",
        "\n",
        "### OK, but why use a Critic?\n",
        "\n",
        "The critic allows us to calculate advantage, so we can use the action-value in the policy loss instead of having to use the observed returns like in REINFORCe. This reduces the variance of our policy, and leads to more stable training.\n",
        "\n",
        "The algorithm we implement here is called Advantage Actor Critic (A2C), and it uses the same loss function as REINFORCE, except it uses the Advantage in place of the observed returns:\n",
        "\n",
        "$\\nabla J(\\theta) = \\log{\\mathbb{P} (a)} * A$\n",
        "\n",
        "Where $A$ is the Advantage estimate.\n",
        "\n",
        "This is again simple to write in code:\n",
        "\n",
        "```\n",
        "policy_loss = -(action_logps * advantages).mean()\n",
        "```\n",
        "\n",
        "Our reason for optimizing the negative loss function is the same as before, ML optimizers like to minimize loss functions, but we need to maximize this one, so we trick the optimizer by backpropagating the negative of the loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "colab_type": "code",
        "id": "ojXV4YBGi9N0",
        "colab": {}
      },
      "source": [
        "class ActorCritic(Actor):\n",
        "    \"\"\"\n",
        "    An Actor-Critic class for A2C.\n",
        "    \n",
        "    Args:\n",
        "        state_space: Actual state space from the environment. Is a gym.spaces type.\n",
        "        hidden_sizes: Hidden layer sizes for MLP policy.\n",
        "        action_space: Action space from the environment. Is a gym.spaces type.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_space: gym.spaces.Space, # environment state space\n",
        "        hidden_sizes: tuple, # hidden layer sizes for MLP policy\n",
        "        action_space: gym.spaces.Space # action space from the environment\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        state_size = state_space.shape[0] # get state size to pass to policy \n",
        "\n",
        "        # Check to be sure that the environment action space is compatible with the Categorical policy.\n",
        "        if isinstance(action_space, gym.spaces.Discrete):\n",
        "            action_dim = action_space.n\n",
        "            self.policy = CategoricalPolicy(state_size, hidden_sizes, action_dim) # make policy\n",
        "        else:\n",
        "            raise ValueError(f\"Env has action space of type {type(action_space)}. REINFORCE Actor currently only supports gym.spaces.Discrete action spaces!\")\n",
        "\n",
        "        self.value_f = MLP(state_size, list(hidden_sizes), 1) # make MLP value function. Its output size is 1 because it should output a single number for expected future return.\n",
        "\n",
        "    def step(self, state):\n",
        "        \"\"\"\n",
        "        Function to get action, logprob of action, and state-value estimate from the actor-critic at each environment timestep.\n",
        "        \n",
        "        Args:\n",
        "            state: Current state of the environment.\n",
        "            \n",
        "        Returns:\n",
        "            action: Selected action\n",
        "            action_logp: Log probability of the selected action\n",
        "            value: estimated state-value of the input state\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            value = self.value_f(state)\n",
        "            policy_current = self.policy.action_distribution(state)\n",
        "            action = policy_current.sample()\n",
        "            action_logp = self.policy.logprob_from_distribution(policy_current, action)\n",
        "        return action.numpy(), action_logp.numpy(), value.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "colab_type": "code",
        "id": "-3j8eXvTiI__",
        "colab": {}
      },
      "source": [
        "class A2C(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Class for training the A2C algorithm on an env.\n",
        "    \n",
        "    Args:\n",
        "        hparams: Namespace object containing all parameters.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        hparams\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hparams = hparams # register parameters to pytorch lightning\n",
        "\n",
        "        hids = (int(i) for i in hparams.hidden_layers) # make sure hidden layer sizes is a tuple of ints\n",
        "\n",
        "        torch.manual_seed(hparams.seed) # random seeding\n",
        "        np.random.seed(hparams.seed)\n",
        "\n",
        "        self.env = gym.make(hparams.env_name) # make RL environment\n",
        "\n",
        "        self.actor_critic = ActorCritic(self.env.observation_space, hids, self.env.action_space) # create actor critic\n",
        "\n",
        "        # initialize buffer\n",
        "        self.buffer = PolicyGradientBuffer(\n",
        "            self.env.observation_space.shape[0], \n",
        "            self.env.action_space.shape,\n",
        "            size=hparams.steps_per_epoch,\n",
        "            gamma=hparams.gamma,\n",
        "            lam=hparams.lam\n",
        "            )\n",
        "\n",
        "        # register parameters to class\n",
        "        self.steps_per_epoch = hparams.steps_per_epoch\n",
        "        self.policy_lr = hparams.policy_lr # policy optimizer lr\n",
        "        self.value_f_lr = hparams.value_f_lr # value function optimizer lr\n",
        "        self.train_iters = hparams.train_iters\n",
        "\n",
        "        self.tracker_dict = {} # init empty metric tracker dictionary\n",
        "\n",
        "        self.inner_loop() # create first batch of data!!!\n",
        "\n",
        "        # set minibatch size for dataloader usage\n",
        "        self.minibatch_size = self.steps_per_epoch // 10\n",
        "        if hparams.minibatch_size is not None:\n",
        "            self.minibatch_size = hparams.minibatch_size\n",
        "\n",
        "    def forward(self, state: torch.Tensor, a: torch.Tensor = None) -> torch.Tensor:\n",
        "        r\"\"\"\n",
        "        Forward pass for the agent.\n",
        "\n",
        "        Args:\n",
        "            state (PyTorch Tensor): state of the environment\n",
        "            a (PyTorch Tensor): action agent took. Optional. Defaults to None.\n",
        "        \"\"\"\n",
        "        return self.actor_critic.policy(state, a) \n",
        "\n",
        "    def configure_optimizers(self) -> tuple:\n",
        "        r\"\"\"\n",
        "        Set up optimizers for agent.\n",
        "\n",
        "        Returns:\n",
        "            policy_optimizer (torch.optim.Adam): Optimizer for policy network.\n",
        "            value_optimizer (torch.optim.Adam): Optimizer for value network.\n",
        "        \"\"\"\n",
        "        self.policy_optimizer = torch.optim.Adam(self.actor_critic.policy.parameters(), lr=self.policy_lr)\n",
        "        self.value_optimizer = torch.optim.Adam(self.actor_critic.value_f.parameters(), lr=self.value_f_lr)\n",
        "        return self.policy_optimizer, self.value_optimizer\n",
        "\n",
        "    def inner_loop(self) -> None:\n",
        "        r\"\"\"\n",
        "        Run agent-env interaction loop. \n",
        "\n",
        "        Stores agent environment interaction tuples to the buffer. Logs reward mean/std/min/max to tracker dict. Collects data at loop end.\n",
        "\n",
        "        \"\"\"\n",
        "        state, reward, episode_reward, episode_length = self.env.reset(), 0, 0, 0 # set state, reward, episode reward/length to initial values\n",
        "        rewlst = [] # empty reward tracking list\n",
        "        lenlst = [] # empty episode length tracking list\n",
        "\n",
        "        for i in range(self.steps_per_epoch): # collect data batch of size steps_per_epoch\n",
        "            action, logp, value = self.actor_critic.step(torch.as_tensor(state, dtype=torch.float32)) # get action, action log prob, and state-value estimate\n",
        "\n",
        "            next_state, reward, done, _ = self.env.step(action) # step environment with action\n",
        "\n",
        "            # store interaction tuple\n",
        "            self.buffer.store(\n",
        "                state,\n",
        "                action,\n",
        "                reward,\n",
        "                value, # this time, we are learning a value function, so we need to store our value estimates for each state. \n",
        "                logp\n",
        "            )\n",
        "\n",
        "            state = next_state # update state\n",
        "            episode_length += 1 # increment episode length\n",
        "            episode_reward += reward # increment episode rewad\n",
        "\n",
        "\n",
        "            timeup = episode_length == 1000 # check if episode has reached maximum length of 1000\n",
        "            over = done or timeup # check if episode is actually over (done == True) or if timeup\n",
        "            epoch_ended = i == self.steps_per_epoch - 1 # check if we've reached the end of the epoch\n",
        "            if over or epoch_ended:\n",
        "                if timeup or epoch_ended:\n",
        "                    # if the epoch has ended or the max episode length has been hit before the episode is over, estimate future returns using the value function\n",
        "                    last_val = self.actor_critic.value_f(torch.as_tensor(state, dtype=torch.float32)).detach().numpy()\n",
        "                else:\n",
        "                    # otherwise, the episode ended properly and we don't need to estimate future return\n",
        "                    last_val = 0\n",
        "                self.buffer.finish_path(last_val) # properly store the finished epoch in the buffer\n",
        "\n",
        "                if over:\n",
        "                    # update tracker lists\n",
        "                    rewlst.append(episode_reward)\n",
        "                    lenlst.append(episode_length)\n",
        "                state, episode_reward, episode_length = self.env.reset(), 0, 0 # reset state and other variables to intial values\n",
        "\n",
        "        # track epoch return and length metrics\n",
        "        trackit = {\n",
        "            \"MeanEpReturn\": np.mean(rewlst),\n",
        "            \"StdEpReturn\": np.std(rewlst),\n",
        "            \"MaxEpReturn\": np.max(rewlst),\n",
        "            \"MinEpReturn\": np.min(rewlst),\n",
        "            \"MeanEpLength\": np.mean(lenlst),\n",
        "            \"Epoch\": self.current_epoch\n",
        "        }\n",
        "        # update class metric tracker dictionary\n",
        "        self.tracker_dict.update(trackit)\n",
        "\n",
        "        # update data with latest epoch data\n",
        "        self.data = self.buffer.get()\n",
        "\n",
        "    def calc_pol_loss(self, logps, returns) -> torch.Tensor:\n",
        "        r\"\"\"\n",
        "        Loss for A2C policy.\n",
        "        \"\"\"\n",
        "        return -(logps * returns).mean()\n",
        "\n",
        "    def calc_val_loss(self, values, returns) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        MSE loss for value function.\n",
        "        \"\"\"\n",
        "        return ((values - returns)**2).mean()\n",
        "  \n",
        "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "        \"\"\"\n",
        "        Depending on which optimizer is being used (optimizer_idx) update the corresponding network.\n",
        "        \"\"\"\n",
        "        states, acts, advs, rets, logps_old = batch\n",
        "\n",
        "        if optimizer_idx == 0:\n",
        "            pol_loss_old = self.calc_pol_loss(logps_old, advs)\n",
        "\n",
        "            policy, logps = self.actor_critic.policy(states, a=acts)\n",
        "            pol_loss = self.calc_pol_loss(logps, advs)\n",
        "\n",
        "            ent = policy.entropy().mean().item() \n",
        "            kl = (logps_old - logps).mean().item()\n",
        "            delta_pol_loss = (pol_loss - pol_loss_old).item()\n",
        "            log = {\"PolicyLoss\": pol_loss_old.item(), \"DeltaPolLoss\": delta_pol_loss, \"Entropy\": ent, \"KL\": kl}\n",
        "            loss = pol_loss\n",
        "\n",
        "        elif optimizer_idx == 1:\n",
        "            values_old = self.actor_critic.value_f(states)\n",
        "            val_loss_old = self.calc_val_loss(values_old, rets)\n",
        "            # value function can take multiple passes over the input data.\n",
        "            for i in range(self.train_iters):\n",
        "                self.value_optimizer.zero_grad()\n",
        "                values = self.actor_critic.value_f(states)\n",
        "                val_loss = self.calc_val_loss(values, rets)\n",
        "                val_loss.backward()\n",
        "                self.value_optimizer.step()\n",
        "\n",
        "            delta_val_loss = (val_loss - val_loss_old).item()\n",
        "            log = {\"ValueLoss\": val_loss_old.item(), \"DeltaValLoss\": delta_val_loss}\n",
        "            loss = val_loss\n",
        "\n",
        "        self.tracker_dict.update(log)\n",
        "        return {\"loss\": loss, \"log\": log, \"progress_bar\": log}\n",
        "      \n",
        "    def training_step_end(\n",
        "        self,\n",
        "        step_dict: dict\n",
        "    ) -> dict:\n",
        "        r\"\"\"\n",
        "        Method for end of training step. Makes sure that episode reward and length info get added to logger.\n",
        "\n",
        "        Args:\n",
        "            step_dict (dict): dictioanry from last training step.\n",
        "\n",
        "        Returns:\n",
        "            step_dict (dict): dictionary from last training step with episode return and length info from last epoch added to log.\n",
        "        \"\"\"\n",
        "        step_dict['log'] = self.add_to_log_dict(step_dict['log'])\n",
        "        return step_dict\n",
        "\n",
        "    def add_to_log_dict(self, log_dict) -> dict:\n",
        "        r\"\"\"\n",
        "        Adds episode return and length info to logger dictionary.\n",
        "\n",
        "        Args:\n",
        "            log_dict (dict): Dictionary to log to.\n",
        "\n",
        "        Returns:\n",
        "            log_dict (dict): Modified log_dict to include episode return and length info.\n",
        "        \"\"\"\n",
        "        add_to_dict = {\n",
        "            \"MeanEpReturn\": self.tracker_dict[\"MeanEpReturn\"],\n",
        "            \"MaxEpReturn\": self.tracker_dict[\"MaxEpReturn\"],\n",
        "            \"MinEpReturn\": self.tracker_dict[\"MinEpReturn\"],\n",
        "            \"MeanEpLength\": self.tracker_dict[\"MeanEpLength\"]}\n",
        "        log_dict.update(add_to_dict)\n",
        "        return log_dict\n",
        "\n",
        "    def train_dataloader(self) -> DataLoader:\n",
        "        r\"\"\"\n",
        "        Define a PyTorch dataset with the data from the last :func:`~inner_loop` run and return a dataloader.\n",
        "\n",
        "        Returns:\n",
        "            dataloader (PyTorch Dataloader): Object for loading data collected during last epoch.\n",
        "        \"\"\"\n",
        "        dataset = PolicyGradientRLDataset(self.data)\n",
        "        dataloader = DataLoader(dataset, batch_size=self.minibatch_size, sampler=None, num_workers=4)\n",
        "        return dataloader\n",
        "\n",
        "    def printdict(self, out_file: Optional[str] = sys.stdout) -> None:\n",
        "        r\"\"\"\n",
        "        Print the contents of the epoch tracking dict to stdout or to a file.\n",
        "\n",
        "        Args:\n",
        "            out_file (sys.stdout or string): File for output. If writing to a file, opening it for writing should be handled in :func:`on_epoch_end`.\n",
        "        \"\"\"\n",
        "        self.print(\"\\n\", file=out_file)\n",
        "        for k, v in self.tracker_dict.items():\n",
        "            self.print(f\"{k}: {v}\", file=out_file)\n",
        "        self.print(\"\\n\", file=out_file)\n",
        "  \n",
        "    def on_epoch_end(self) -> None:\n",
        "        r\"\"\"\n",
        "        Print tracker_dict, reset tracker_dict, and generate new data with inner loop.\n",
        "        \"\"\"\n",
        "        self.printdict()\n",
        "        self.tracker_dict = {}\n",
        "        self.inner_loop()\n",
        "\n",
        "    def optimizer_step(\n",
        "        self,\n",
        "        epoch,\n",
        "        batch_idx,\n",
        "        optimizer,\n",
        "        optimizer_idx,\n",
        "        second_order_closure=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        For compatibility with PyTorch Lightning, need to ignore optimizer step for value function optimizer because it is done in training step.\n",
        "        \"\"\"\n",
        "        if optimizer_idx == 0:\n",
        "            if self.trainer.use_tpu and XLA_AVAILABLE:\n",
        "                xm.optimizer_step(optimizer)\n",
        "            elif isinstance(optimizer, torch.optim.LBFGS):\n",
        "                optimizer.step(second_order_closure)\n",
        "            else:\n",
        "                optimizer.step()\n",
        "\n",
        "                # clear gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "        elif optimizer_idx == 1:\n",
        "            pass\n",
        "\n",
        "    def backward(\n",
        "        self,\n",
        "        trainer,\n",
        "        loss,\n",
        "        optimizer,\n",
        "        optimizer_idx\n",
        "    ):\n",
        "        \"\"\"\n",
        "        For compatibility with PyTorch Lightning, need to ignore backward pass for value function because it is done in training step.D\n",
        "        \"\"\"\n",
        "        if optimizer_idx == 0:\n",
        "            if trainer.precision == 16:\n",
        "                # .backward is not special on 16-bit with TPUs\n",
        "                if not trainer.on_tpu:\n",
        "                    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                        scaled_loss.backward()\n",
        "            else:\n",
        "                loss.backward()\n",
        "\n",
        "        elif optimizer_idx == 1:\n",
        "            pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "colab_type": "code",
        "id": "U6EE2dcEoKTG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f0a781cd93374b3f88158eb60bc01238",
            "e5f25ca13bee4937a97abbf25ff8faa6",
            "810e166708814013be4bae3fd8f6c6a9",
            "ce8d18dca93746caba6d355f4eb678a3",
            "a7a7193f67e447929e48bcb3913794b9",
            "41524da1be7945bd82f25d60466aee34",
            "7a88dcb786c846f8b18092382602e92c",
            "8be6d638acfd4219989392d078bdf3f0",
            "4e0cffcd0a9e442d9fca0c659f6d94ef"
          ]
        },
        "outputId": "a9734753-142c-4caa-d1f8-4dd0324791dd"
      },
      "source": [
        "epochs = 100\n",
        "hparams = Namespace(env_name=\"CartPole-v1\", hidden_layers=(64, 32), seed=123, gamma=0.99, lam=0.97, steps_per_epoch=4000, policy_lr=3e-4, value_f_lr=1e-3, minibatch_size=400, train_iters=80)\n",
        "trainer = pl.Trainer(\n",
        "    reload_dataloaders_every_epoch = True,\n",
        "    early_stop_callback = False,\n",
        "    max_epochs = epochs\n",
        ")\n",
        "\n",
        "agent = A2C(hparams)\n",
        "trainer.fit(agent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "\n",
            "  | Name         | Type        | Params\n",
            "---------------------------------------------\n",
            "0 | actor_critic | ActorCritic | 2 K   \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4e0cffcd0a9e442d9fca0c659f6d94ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "MeanEpReturn: 21.56756756756757\n",
            "StdEpReturn: 11.103786888825887\n",
            "MaxEpReturn: 85.0\n",
            "MinEpReturn: 8.0\n",
            "MeanEpLength: 21.56756756756757\n",
            "Epoch: 0\n",
            "PolicyLoss: -0.0475136898458004\n",
            "DeltaPolLoss: -0.012714315205812454\n",
            "Entropy: 0.6899381279945374\n",
            "KL: 0.004414649680256844\n",
            "ValueLoss: 169.20973205566406\n",
            "DeltaValLoss: -1.5006103515625\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 23.239766081871345\n",
            "StdEpReturn: 10.981422268221982\n",
            "MaxEpReturn: 68.0\n",
            "MinEpReturn: 9.0\n",
            "MeanEpLength: 23.239766081871345\n",
            "Epoch: 1\n",
            "PolicyLoss: 0.11571130156517029\n",
            "DeltaPolLoss: -0.0025793761014938354\n",
            "Entropy: 0.6833640933036804\n",
            "KL: -0.002211151644587517\n",
            "ValueLoss: 256.2491455078125\n",
            "DeltaValLoss: -2.2222747802734375\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 27.034246575342465\n",
            "StdEpReturn: 12.559673662740728\n",
            "MaxEpReturn: 79.0\n",
            "MinEpReturn: 9.0\n",
            "MeanEpLength: 27.034246575342465\n",
            "Epoch: 2\n",
            "PolicyLoss: 0.2422216534614563\n",
            "DeltaPolLoss: -0.007701784372329712\n",
            "Entropy: 0.6777065396308899\n",
            "KL: -0.0005352660082280636\n",
            "ValueLoss: 453.5418395996094\n",
            "DeltaValLoss: -3.914337158203125\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MeanEpReturn: 29.548148148148147\n",
            "StdEpReturn: 14.262989411095246\n",
            "MaxEpReturn: 94.0\n",
            "MinEpReturn: 9.0\n",
            "MeanEpLength: 29.548148148148147\n",
            "Epoch: 3\n",
            "PolicyLoss: 0.008651922456920147\n",
            "DeltaPolLoss: -0.005422811955213547\n",
            "Entropy: 0.6700934767723083\n",
            "KL: -0.0006165042286738753\n",
            "ValueLoss: 302.0799255371094\n",
            "DeltaValLoss: -2.21783447265625\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "id": "FqNK2bk3hu4M"
      },
      "source": [
        "# Bonus section:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "id": "mi9dA8HCr9Q2"
      },
      "source": [
        "## Challenge Problems!!!\n",
        "\n",
        "If you've enjoyed this stuff and want to try to learn to do something on your own, I'll list a couple of recommended next steps (\"challenges\") here.\n",
        "\n",
        "1. Implement and train [PPO](https://arxiv.org/abs/1707.06347). The code from A2C doesn't require much modification to be converted into PPO.\n",
        "2. Write policy networks for continuous action spaces. These are commonly called Gaussian policies, and they learn to output the mean of a (Normal) action distribution. Some implementations also learn the log standard devaiation of the distribution, but this isn't necessary here. You can fix the log standard deviation to -0.5 and achieve decent scores on most things.\n",
        "  - Further reading to help with this: \n",
        "    - [SpinningUp](https://spinningup.openai.com/en/latest/)\n",
        "    - [My open-source implementations (very similar to the code we've written here)](https://github.com/jfpettit/flare)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "id": "1DMqskwYYiEb"
      },
      "source": [
        "## Some Extra Reading!\n",
        "\n",
        "For the curious person who wants to go deeper:\n",
        "\n",
        "- [Sutton and Barto's Introduction to RL](http://incompleteideas.net/book/the-book-2nd.html)\n",
        "  - This is really the holy grail of classical RL.\n",
        "- [OpenAI's SpinningUp](https://spinningup.openai.com/en/latest/)\n",
        "  - Another good introduction, doesn't require nearly as much time as the Sutton and Barto book but still provides a good overview of RL.\n",
        "- [Lilian Weng's \"A (Long) Peek Into Reinforcement Learning\"](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html)\n",
        "  - Lilian Weng's blog is in general a great resource, and she has written many blog posts on other RL topics as well."
      ]
    }
  ]
}